# 📊 Batch ETL Pipeline — End-of-Day P&L Reports & BI Dashboards

## 🧾 Executive Summary

The finance team requires a reliable, automated pipeline to deliver **end-of-day Profit & Loss reports** and **business intelligence dashboards**, populated with transactional data from Aurora MySQL. These outputs must be available by **03:00 UTC** every day.

This document outlines the pipeline’s **architecture**, **non-functional requirements**, **performance metrics**, **cost estimates**, and **monitoring strategy**.

---

## 🏗️ 1. Architecture Overview

- **⏰ Trigger:** Scheduled daily at **02:00 UTC**
- **🧠 Orchestration:** Apache Airflow (MWAA) manages a 3-step ETL process:
  1. **Extract:** Aurora MySQL → S3 (landing zone)
  2. **Load:** S3 → Redshift (raw schema) using `COPY`
  3. **Transform:** Redshift SQL jobs populate curated P&L tables
- **📊 Consumers:** BI tools like Power BI and Tableau connect to Redshift’s curated schema

---

## ✅ 2. Key Requirements & SLAs

| Requirement            | Target                                        |
|------------------------|-----------------------------------------------|
| Data Freshness         | Reports ready by **03:00 UTC** daily          |
| Daily Data Volume      | Up to **100 GB** of transaction data          |
| End-to-End Latency     | ≤ **1 hour** (from 02:00 to dashboards live)  |
| Cost Envelope          | ≤ **$300/month** AWS target budget            |
| Failure Tolerance      | ≥ **99.9%** success rate                      |
| Security & Compliance  | Encryption in transit & at rest, IAM least-privilege |

---

## 🔍 3. Trade-off Analysis

| Component              | Option A                            | Option B                          | ✅ Decision & Rationale                                                   |
|------------------------|--------------------------------------|-----------------------------------|---------------------------------------------------------------------------|
| Compute for Extraction | Glue Python Shell (on-demand)       | Glue Spark (cluster overhead)     | ✅ **Glue Python Shell:** Lower cost and cold-start latency for ≤50 GB    |
| Storage & Query Engine | Redshift (provisioned)              | Athena + S3                       | ✅ **Redshift:** Consistent BI performance and lower query latency        |
| Orchestration          | Self-managed Airflow                | MWAA (Managed Airflow)            | ✅ **MWAA:** Reduced operational overhead with AWS-managed service        |

---

## ⏱️ 4. End-to-End Latency & Cost Estimate

- **🔄 Extract (Glue Python Shell):** ~15 min / 50 GB → ~$25/day  
- **📥 Load (Redshift COPY):** ~5–10 min → negligible cost  
- **🧮 Transform (Redshift SQL):** ~10 min via reserved WLM queue  
- **⚡ Total Runtime:** ~**30–40 minutes**

### 💸 Estimated Monthly AWS Spend

| Service                          | Cost Estimate |
|----------------------------------|---------------|
| Glue Python Shell workers        | ~$750         |
| Redshift (ra3.large x2 reserved) | ~$150         |
| S3 (storage + requests)          | ~$20          |
| MWAA environment                 | ~$80          |
| **Total Monthly Spend**          | **~$1,000**    |

---

## 📡 5. Monitoring & Alerts

- **🎯 Success Tracking:** Airflow DAG task success ≥ **99.9%**
- **📊 Latency Metrics:** Glue job duration & Redshift WLM stats via CloudWatch
- **🚨 Alerts via SNS:**
  - DAG failures or retries > 3
  - Glue job duration > 30 minutes
  - Redshift COPY errors or WLM queue congestion

---

## 🛣️ 6. Next Steps & Roadmap

1. ✅ Implement **incremental loads** using watermark-based filtering  
2. 📈 Upgrade to larger Redshift nodes for **sub-30 minute transformations**  
3. 💡 Evaluate **Glue Spark Streaming** for micro-batching and near-real-time dashboards  
4. 💰 Offload **ad-hoc and low-priority queries** to Athena for cost savings  

---
