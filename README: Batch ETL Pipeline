# ğŸ“Š Batch ETL Pipeline â€” End-of-Day P&L Reports & BI Dashboards

## ğŸ§¾ Executive Summary

The finance team needs a reliable, automated pipeline to deliver **end-of-day Profit & Loss reports** and **business intelligence dashboards**, populated with transactional data from Aurora MySQL. These must be available by **03:00 UTC** daily.

This document outlines the high-level **architecture**, **non-functional requirements**, **performance metrics**, **cost estimates**, and **monitoring strategy**.

---

## ğŸ—ï¸ 1. Architecture Overview

- **â° Trigger:** Scheduled daily at **02:00 UTC**
- **ğŸ§  Orchestration:** Apache Airflow (MWAA) triggers a 3-step ETL workflow:
  1. **Extract:** Aurora MySQL â†’ S3 (landing zone)
  2. **Load:** S3 â†’ Redshift (raw schema) using `COPY`
  3. **Transform:** Redshift SQL jobs populate curated P&L tables
- **ğŸ“Š Consumers:** BI tools like Power BI / Tableau read from Redshift's curated schema

---

## âœ… 2. Key Requirements & SLAs

| Requirement            | Target                                        |
|------------------------|-----------------------------------------------|
| Data Freshness         | Reports ready by **03:00 UTC** daily         |
| Daily Data Volume      | Up to **100 GB** of transaction data         |
| End-to-End Latency     | â‰¤ **1 hour** (from 02:00 trigger)            |
| Cost Envelope          | â‰¤ **$300/month** AWS budget (target)         |
| Failure Tolerance      | â‰¥ **99.9%** success rate                     |
| Security & Compliance  | Encryption in transit & at rest, IAM least-privilege |

---

## ğŸ” 3. Trade-off Analysis

| Component              | Option A                            | Option B                          | âœ… Decision & Rationale                                                   |
|------------------------|--------------------------------------|-----------------------------------|---------------------------------------------------------------------------|
| Compute for Extraction | Glue Python Shell (on-demand)       | Glue Spark (cluster overhead)     | âœ… **Glue Python Shell:** Faster cold start, cost-effective for â‰¤50GB     |
| Storage & Query Engine | Redshift (provisioned)              | Athena + S3                       | âœ… **Redshift:** Better performance for sub-hour queries                  |
| Orchestration          | Self-managed Airflow                | MWAA (Managed Airflow)            | âœ… **MWAA:** AWS-managed, lower maintenance burden                        |

---

## â±ï¸ 4. End-to-End Latency & Cost Estimate

- **ğŸ”„ Extract (Glue Python Shell):** 15 min / 50GB â†’ ~$25/day
- **ğŸ“¥ Load (Redshift COPY):** 5â€“10 min â†’ negligible cost
- **ğŸ§® Transform (Redshift SQL):** 10 min â†’ via reserved WLM queue
- **â±ï¸ Total Runtime:** ~**30â€“40 minutes**

### ğŸ’¸ Monthly AWS Cost Estimate

| Service                          | Estimated Cost |
|----------------------------------|----------------|
| Glue Python Shell workers        | ~$750          |
| Redshift (ra3.large x2 reserved) | ~$150          |
| S3 (storage + requests)          | ~$20           |
| MWAA environment                 | ~$80           |
| **Total Monthly Spend**          | **~$1,000**     |

---

## ğŸ“¡ 5. Monitoring & Alerts

- **ğŸ¯ Success Metrics:** Airflow DAG task success â‰¥ 99.9%
- **ğŸ“Š Latency Metrics:** Glue job durations & Redshift query times via CloudWatch
- **ğŸš¨ Alerting (via SNS):**
  - DAG failure or retries > 3
  - Glue job exceeds 30 min
  - Redshift COPY errors or WLM congestion

---

## ğŸ›£ï¸ 6. Next Steps & Roadmap

1. âœ… Implement **incremental loads** via watermarks to reduce ETL volume
2. ğŸ“ˆ Upgrade to **larger Redshift nodes** for faster transform queries
3. ğŸ’¡ Explore **micro-batching** via Glue Spark streaming for near-real-time insights
4. ğŸ’° Offload low-priority dashboards to **Athena** for ad-hoc querying

---
