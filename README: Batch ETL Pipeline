# ğŸ“Š Batch ETL Pipeline â€” End-of-Day P&L Reports & BI Dashboards

## ğŸ§¾ Executive Summary

The finance team requires a reliable, automated pipeline to deliver **end-of-day Profit & Loss reports** and **business intelligence dashboards**, populated with transactional data from Aurora MySQL. These outputs must be available by **03:00 UTC** every day.

This document outlines the pipelineâ€™s **architecture**, **non-functional requirements**, **performance metrics**, **cost estimates**, and **monitoring strategy**.

---

## ğŸ—ï¸ 1. Architecture Overview

- **â° Trigger:** Scheduled daily at **02:00 UTC**
- **ğŸ§  Orchestration:** Apache Airflow (MWAA) manages a 3-step ETL process:
  1. **Extract:** Aurora MySQL â†’ S3 (landing zone)
  2. **Load:** S3 â†’ Redshift (raw schema) using `COPY`
  3. **Transform:** Redshift SQL jobs populate curated P&L tables
- **ğŸ“Š Consumers:** BI tools like Power BI and Tableau connect to Redshiftâ€™s curated schema

---

## âœ… 2. Key Requirements & SLAs

| Requirement            | Target                                        |
|------------------------|-----------------------------------------------|
| Data Freshness         | Reports ready by **03:00 UTC** daily          |
| Daily Data Volume      | Up to **100â€¯GB** of transaction data          |
| End-to-End Latency     | â‰¤ **1 hour** (from 02:00 to dashboards live)  |
| Cost Envelope          | â‰¤ **$300/month** AWS target budget            |
| Failure Tolerance      | â‰¥ **99.9%** success rate                      |
| Security & Compliance  | Encryption in transit & at rest, IAM least-privilege |

---

## ğŸ” 3. Trade-off Analysis

| Component              | Option A                            | Option B                          | âœ… Decision & Rationale                                                   |
|------------------------|--------------------------------------|-----------------------------------|---------------------------------------------------------------------------|
| Compute for Extraction | Glue Python Shell (on-demand)       | Glue Spark (cluster overhead)     | âœ… **Glue Python Shell:** Lower cost and cold-start latency for â‰¤50â€¯GB    |
| Storage & Query Engine | Redshift (provisioned)              | Athena + S3                       | âœ… **Redshift:** Consistent BI performance and lower query latency        |
| Orchestration          | Self-managed Airflow                | MWAA (Managed Airflow)            | âœ… **MWAA:** Reduced operational overhead with AWS-managed service        |

---

## â±ï¸ 4. End-to-End Latency & Cost Estimate

- **ğŸ”„ Extract (Glue Python Shell):** ~15 min / 50â€¯GB â†’ ~$25/day  
- **ğŸ“¥ Load (Redshift COPY):** ~5â€“10 min â†’ negligible cost  
- **ğŸ§® Transform (Redshift SQL):** ~10 min via reserved WLM queue  
- **âš¡ Total Runtime:** ~**30â€“40 minutes**

### ğŸ’¸ Estimated Monthly AWS Spend

| Service                          | Cost Estimate |
|----------------------------------|---------------|
| Glue Python Shell workers        | ~$750         |
| Redshift (ra3.large x2 reserved) | ~$150         |
| S3 (storage + requests)          | ~$20          |
| MWAA environment                 | ~$80          |
| **Total Monthly Spend**          | **~$1,000**    |

---

## ğŸ“¡ 5. Monitoring & Alerts

- **ğŸ¯ Success Tracking:** Airflow DAG task success â‰¥ **99.9%**
- **ğŸ“Š Latency Metrics:** Glue job duration & Redshift WLM stats via CloudWatch
- **ğŸš¨ Alerts via SNS:**
  - DAG failures or retries > 3
  - Glue job duration > 30 minutes
  - Redshift COPY errors or WLM queue congestion

---

## ğŸ›£ï¸ 6. Next Steps & Roadmap

1. âœ… Implement **incremental loads** using watermark-based filtering  
2. ğŸ“ˆ Upgrade to larger Redshift nodes for **sub-30 minute transformations**  
3. ğŸ’¡ Evaluate **Glue Spark Streaming** for micro-batching and near-real-time dashboards  
4. ğŸ’° Offload **ad-hoc and low-priority queries** to Athena for cost savings  

---
